{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\rashi agarwal\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Error parsing dependencies of pytorch-lightning: .* suffix can only be used with `==` or `!=` operators\n",
      "    torch (>=1.8.*)\n",
      "           ~~~~~~^\n",
      "WARNING: Error parsing dependencies of textract: .* suffix can only be used with `==` or `!=` operators\n",
      "    extract-msg (<=0.29.*)\n",
      "                 ~~~~~~~^\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\rashi agarwal\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.1.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pandas sqlalchemy psycopg2 -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQLAlchemy engine created successfully.\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Define database connection parameters\n",
    "db_username = 'postgres'       \n",
    "db_password = '*****'       # actual password removed while submitting\n",
    "db_host = 'localhost'              \n",
    "db_port = '5432'                  \n",
    "db_name = 'ecommerce_db'           \n",
    "\n",
    "# Construct the connection URL\n",
    "db_url = f'postgresql://{db_username}:{db_password}@{db_host}:{db_port}/{db_name}'\n",
    "\n",
    "# Create the SQLAlchemy engine\n",
    "engine = create_engine(db_url)\n",
    "\n",
    "print(\"SQLAlchemy engine created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Engine(postgresql://postgres:***@localhost:5432/ecommerce_db)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rashi Agarwal\\AppData\\Local\\Temp\\ipykernel_17648\\1782640439.py:9: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)\n",
      "  Base = declarative_base()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import Column, Integer, String, Float, Text, DateTime\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from datetime import datetime\n",
    "\n",
    "#Create a Database Schema:\n",
    "# Define the schema\n",
    "Base = declarative_base()\n",
    "\n",
    "#sample dataset containing both structured and unstructured data\n",
    "#Structured Data: user_id, timestamp, location, product_id, price are all structured fields that can easily be queried in a database.\n",
    "#Unstructured Data: review_text is an unstructured text field \n",
    "\n",
    "class ProductReview(Base):\n",
    "    __tablename__ = 'product_reviews'\n",
    "    id = Column(Integer, primary_key=True, autoincrement=True)\n",
    "    user_id = Column(Integer, nullable=False)\n",
    "    timestamp = Column(DateTime, nullable=False)\n",
    "    location = Column(String, nullable=False)\n",
    "    product_id = Column(Integer, nullable=False)\n",
    "    product_name = Column(String, nullable=False)\n",
    "    price = Column(Float, nullable=False)\n",
    "    review_text = Column(Text, nullable=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the table\n",
    "Base.metadata.create_all(engine)\n",
    "\n",
    "# Create a session\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from CSV\n",
    "df = pd.read_csv('ecommerce_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Preprocess 'timestamp' field to ensure proper format\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "# Insert the data into the database\n",
    "for index, row in df.iterrows():\n",
    "    review = ProductReview(\n",
    "        user_id=row['user_id'],\n",
    "        timestamp=row['timestamp'],\n",
    "        location=row['location'],\n",
    "        product_id=row['product_id'],\n",
    "        product_name=row['product_name'],\n",
    "        price=row['price'],\n",
    "        review_text=row['review_text']\n",
    "    )\n",
    "    session.add(review)\n",
    "\n",
    "# Commit the session to save to the database\n",
    "session.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index('idx_location', Column('location', String(), table=<product_reviews>, nullable=False))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sqlalchemy import Index\n",
    "\n",
    "# Create indexes on frequently queried columns\n",
    "Index('idx_user_id', ProductReview.user_id)\n",
    "Index('idx_product_id', ProductReview.product_id)\n",
    "Index('idx_timestamp', ProductReview.timestamp)\n",
    "Index('idx_location', ProductReview.location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User 1 from New York says: Great phone, very fast and sleek design!\n",
      "User 3 from Chicago says: Battery life could be better.\n",
      "User 1 from New York says: Great phone, very fast and sleek design!\n",
      "User 3 from Chicago says: Battery life could be better.\n",
      "User 1 from New York says: Great phone, very fast and sleek design!\n",
      "User 3 from Chicago says: Battery life could be better.\n"
     ]
    }
   ],
   "source": [
    "# Querying the data\n",
    "result = session.query(ProductReview).filter(ProductReview.product_id == 101).all()\n",
    "\n",
    "for review in result:\n",
    "    print(f\"User {review.user_id} from {review.location} says: {review.review_text}\")\n",
    "\n",
    "#multiple run of cells have resulted in below output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values After Preprocessing:\n",
      "user_id         0\n",
      "timestamp       0\n",
      "location        0\n",
      "product_id      0\n",
      "product_name    0\n",
      "price           0\n",
      "review_text     0\n",
      "dtype: int64\n",
      "   user_id           timestamp       location  product_id product_name  \\\n",
      "0        1 2024-11-01 10:00:00       new york         101   Smartphone   \n",
      "1        2 2024-11-01 10:30:00    los angeles         102       Laptop   \n",
      "2        3 2024-11-02 09:45:00        chicago         101   Smartphone   \n",
      "3        4 2024-11-02 11:00:00  san francisco         103       Tablet   \n",
      "4        5 2024-11-03 08:30:00         boston         104   Smartwatch   \n",
      "\n",
      "    price                                        review_text  \n",
      "0  599.99           great phone, very fast and sleek design!  \n",
      "1  999.99                the laptop is good but a bit heavy.  \n",
      "2  599.99                      battery life could be better.  \n",
      "3  299.99     excellent tablet for reading and web browsing.  \n",
      "4  199.99  not the best, but works well for basic functions.  \n"
     ]
    }
   ],
   "source": [
    "# 1. **Text Cleaning for `review_text`**\n",
    "#other cleaning like stopwords, punctuation , stemming, lemmetization can also be performed\n",
    "\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):\n",
    "        # Remove non-alphanumeric characters except spaces and punctuation\n",
    "        text = re.sub(r'[^\\w\\s,.?!]', '', text)\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "    return text\n",
    "\n",
    "df['review_text'] = df['review_text'].apply(clean_text)\n",
    "\n",
    "# 2. **Handle Missing Values**\n",
    "# For numerical columns, we fill missing values with the mean of the column\n",
    "df['price'].fillna(df['price'].mean(), inplace=True)\n",
    "\n",
    "# For text columns, we fill missing values with 'No review'\n",
    "df['review_text'].fillna('No review', inplace=True)\n",
    "\n",
    "# For datetime columns, we parse the 'timestamp' column into a datetime object\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "\n",
    "# 3. **Check for missing values after preprocessing**\n",
    "missing_values = df.isnull().sum()\n",
    "\n",
    "# 4. **Normalization (optional)**: Standardize the 'location' column to lowercase\n",
    "df['location'] = df['location'].str.lower()\n",
    "\n",
    "# 5. **Handle Noise (optional)**: For simplicity, let's just check for erroneous data\n",
    "df = df[df['price'] > 0]  # Remove rows where price is non-positive\n",
    "\n",
    "# 6. **Preprocessed Data Inspection**\n",
    "print(\"Missing Values After Preprocessing:\")\n",
    "print(missing_values)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into the PostgreSQL database\n",
    "df.to_sql('product_reviews', engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\rashi agarwal\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Error parsing dependencies of pytorch-lightning: .* suffix can only be used with `==` or `!=` operators\n",
      "    torch (>=1.8.*)\n",
      "           ~~~~~~^\n",
      "WARNING: Error parsing dependencies of textract: .* suffix can only be used with `==` or `!=` operators\n",
      "    extract-msg (<=0.29.*)\n",
      "                 ~~~~~~~^\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\rashi agarwal\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\rashi agarwal\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "#hugging face sentence transformers used for generating text embeddings\n",
    "pip install sentence_transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the review_text column into embeddings using a pre-trained language model like Sentence-Transformers\n",
    "#balancing speed and accuracy\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load pre-trained model\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# Convert reviews to embeddings\n",
    "embeddings = model.encode(df['review_text'].tolist())\n",
    "\n",
    "# Store the embeddings along with the product_id\n",
    "df['embedding'] = embeddings.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\rashi agarwal\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Error parsing dependencies of pytorch-lightning: .* suffix can only be used with `==` or `!=` operators\n",
      "    torch (>=1.8.*)\n",
      "           ~~~~~~^\n",
      "WARNING: Error parsing dependencies of textract: .* suffix can only be used with `==` or `!=` operators\n",
      "    extract-msg (<=0.29.*)\n",
      "                 ~~~~~~~^\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\rashi agarwal\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Rashi Agarwal\\AppData\\Local\\Programs\\Python\\Python39\\Lib\\site-packages\\google\\~~otobuf'.\n",
      "  You can safely remove it manually.\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\rashi agarwal\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "mediapipe 0.10.5 requires protobuf<4,>=3.11, but you have protobuf 4.25.5 which is incompatible.\n",
      "tensorboard 2.11.0 requires protobuf<4,>=3.9.2, but you have protobuf 4.25.5 which is incompatible.\n",
      "tensorflow-intel 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.25.5 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "#Pinecone is used to store the embeddings for efficient similarity search. Fully managed storage solution for vector databases\n",
    "#cloud based\n",
    "#highly scalable\n",
    "#using serverless pinecone instance\n",
    "pip install \"pinecone[grpc]\" -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_api_key = 'pcsk_***' # api key for pinecone removed while submitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'upserted_count': 5}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "# Save the embeddings in a vector store (e.g., Pinecone)\n",
    "\n",
    "\n",
    "# Initialize Pinecone connection\n",
    "pc = Pinecone(api_key= pc_api_key)\n",
    "\n",
    "# Create an index (if not exists)\n",
    "index_name = 'product-review-1'\n",
    "    # Now do stuff\n",
    "if index_name not in pc.list_indexes().names():\n",
    "        pc.create_index(\n",
    "            name=index_name,\n",
    "            dimension=embeddings.shape[1],\n",
    "            metric='euclidean',\n",
    "            spec=ServerlessSpec(\n",
    "                cloud='aws',\n",
    "                region='us-east-1'\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "# Connect to the index\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "\n",
    "# Insert the embeddings into Pinecone (batch processing)\n",
    "#The embeddings are stored alongside the product_id so that they can be retrieved later along with the product details.\n",
    "\n",
    "\n",
    "vectors = [(str(i), embedding) for i, embedding in enumerate(embeddings)]\n",
    "\n",
    "index.upsert(vectors = vectors, namespace=\"assignment-namespace\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.Query and Retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'assignment-namespace': {'vector_count': 5}},\n",
      " 'total_vector_count': 5}\n"
     ]
    }
   ],
   "source": [
    "print(index.describe_index_stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the vector store (Pinecone)\n",
    "index_name = 'product-review-1'\n",
    "\n",
    "# Example query\n",
    "query = \"How is the smartphone battery?\"\n",
    "\n",
    "\n",
    "#Convert the query into a numerical vector that Pinecone can search with\n",
    "#emebddings size is 1024 here while the one generated for index is 384\n",
    "query_embedding = pc.inference.embed(\n",
    "    model=\"multilingual-e5-large\",\n",
    "    inputs=[query],\n",
    "    parameters={\n",
    "        \"input_type\": \"query\"\n",
    "    }\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'matches': [{'id': '4', 'score': 30.0337715, 'values': []},\n",
      "             {'id': '1', 'score': 41.1877975, 'values': []},\n",
      "             {'id': '3', 'score': 50.2384796, 'values': []}],\n",
      " 'namespace': 'assignment-namespace',\n",
      " 'usage': {'read_units': 6}}\n"
     ]
    }
   ],
   "source": [
    "#trucating query embedding from 1024 to 384 to match our index dimensions\n",
    "#query_embedding = query_embedding[:384]\n",
    "\n",
    "# Search the index for the three most similar vectors\n",
    "results = index.query(\n",
    "    namespace=\"assignment-namespace\",\n",
    "    vector=query_embedding[0].values[:384],\n",
    "    top_k=3,\n",
    "    include_values=False,\n",
    "    include_metadata=True\n",
    ")\n",
    "\n",
    "print(results)\n",
    "#scores are large due to directly trucating the emebddings . could be handled better and index embeddings can also be generated using PINECONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Not the best, but works well for basic functions. The laptop is good but a bit heavy. Excellent tablet for reading and web browsing.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ind =[int(match['id']) for match in results['matches']]\n",
    "# similar_reviews = df['review_text'][ind].tolist()\n",
    "# combined_reviews = \" \".join([r for r in similar_reviews])\n",
    "# combined_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retriever-Augmented Generation (RAG),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63d27a7e8a964643a4ff8b58b993a4df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rashi Agarwal\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Rashi Agarwal\\.cache\\huggingface\\hub\\models--google--flan-t5-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79815c9f2d7d4da3bd2cf7e088447f3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1334bfbab8d24acca8b197c9acfa2dba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f56ef790b5e4b98a4d1d6d3933842e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01d78e741dac498b9faf1b893cce14d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d4e6774b62147859da440a653079aa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4e316f6c78841bd9e74c3737c2a4cee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Response: Not the best\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Load pre-trained generative model (google/flan-t5-base)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "ind =[int(match['id']) for match in results['matches']]\n",
    "all_reviews = df['review_text'][ind].tolist()\n",
    "combined_reviews = \" \".join([r for r in all_reviews])\n",
    "\n",
    "# Combine query and retrieved context\n",
    "context = combined_reviews\n",
    "input_text = f\"question: {query} context: {context}\"\n",
    "\n",
    "# Tokenize and generate response\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "outputs = model.generate(input_ids, max_length=50, num_beams=5, early_stopping=True)\n",
    "\n",
    "# Decode and print the answer\n",
    "answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Generated Response:\", answer)\n",
    "\n",
    "#Generated Response: Not the best  -- shown below. this is in line with our original reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
